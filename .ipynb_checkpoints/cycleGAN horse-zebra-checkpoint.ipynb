{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "import matplotlib.pyplot as plt\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    image_list=[]\n",
    "    for filename in os.listdir(path):\n",
    "        image=load_img(os.path.join(path,filename))\n",
    "        image=img_to_array(image) #to ndarray\n",
    "        image_list.append(image)\n",
    "    return np.asarray(image_list,'float') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import PIL.Image. The use of `load_img` requires PIL.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-edb95d1fb517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainA\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'horse2zebra//trainA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtestA\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'horse2zebra\\\\testA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'horse2zebra\\\\trainB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtestB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'horse2zebra\\\\testB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-add4bf51dde6>\u001b[0m in \u001b[0;36mload_image\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mimage_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#to ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mimage_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2.0/lib/python3.6/site-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mcolor_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpil_image\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         raise ImportError('Could not import PIL.Image. '\n\u001b[0m\u001b[1;32m    109\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[1;32m    110\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import PIL.Image. The use of `load_img` requires PIL."
     ]
    }
   ],
   "source": [
    "trainA=load_image('horse2zebra//trainA')\n",
    "testA=load_image('horse2zebra\\\\testA')\n",
    "trainB=load_image('horse2zebra\\\\trainB')\n",
    "testB=load_image('horse2zebra\\\\testB')\n",
    "A=np.vstack((trainA,testA))\n",
    "B=np.vstack((trainB,testB))\n",
    "print(A.shape,B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randi=np.random.randint(0,A.shape[0],25)\n",
    "randA=A[randi]\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.imshow(randA[i].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randi=np.random.randint(0,B.shape[0],25)\n",
    "randB=B[randi]\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.imshow(randB[i].astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator NN  \n",
    "![cycleGAN](https://1.bp.blogspot.com/-cj53ochEvkY/XVPRc4F9A0I/AAAAAAAAEbQ/ERiS_nhQpPEDZOOk8SrxmQ6E2CdZFXMTACLcBGAs/s640/%25E6%2593%25B7%25E5%258F%2596.PNG)  \n",
    "implement the PatchGAN discriminator  \n",
    "\n",
    "GAN一般情況下的網絡結構，在一些人的實驗中已經表明對於要求高分辨率、高細節保持的圖像領域中並不適合，有些人根據這一情況設計了PatchGAN的思路。這種GAN的差別主要是在於Discriminator上，一般的GAN是只需要輸出一個true or fasle 的矢量，這是代表對整張圖像的評價；但是PatchGAN輸出的是一個N x N的矩陣，這個N x N的矩陣的每一個元素，比如a(i,j) 只有True or False 這兩個選擇（label 是N x N的矩陣，每一個元素是True 或者False），這樣的結果往往是通過卷積層來達到的，因為逐次疊加的捲積層最終輸出的這個N x N 的矩陣，其中的每一個元素，實際上代表著原圖中的一個比較大的感受野，也就是說對應著原圖中的一個Patch，因此具有這樣結構以及這樣輸出的GAN被稱之為Patch GAN。 \n",
    "\n",
    "Native keras doesn't go well with tf.keras. 若用tf，則InstanceNormalization會無法執行。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(image_shape):\n",
    "    init=keras.initializers.RandomNormal(stddev=0.02) #normal distribution\n",
    "    x_input=keras.layers.Input(shape=image_shape)\n",
    "    #to(128*128*64)\n",
    "    x=keras.layers.Conv2D(64,(4,4),strides=(2,2),padding='same',kernel_initializer=init)(x_input)\n",
    "    x=keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    #to(64*64*128)\n",
    "    x=keras.layers.Conv2D(128,(4,4),strides=(2,2),padding='same',kernel_initializer=init)(x)\n",
    "    x=InstanceNormalization(axis=-1)(x)\n",
    "    x=keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    #to(32*32*256)\n",
    "    x=keras.layers.Conv2D(256,(4,4),strides=(2,2),padding='same',kernel_initializer=init)(x)\n",
    "    x=InstanceNormalization(axis=-1)(x)\n",
    "    x=keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    #to(16*16*512)\n",
    "    x=keras.layers.Conv2D(512,(4,4),strides=(2,2),padding='same',kernel_initializer=init)(x)\n",
    "    x=InstanceNormalization(axis=-1)(x)\n",
    "    x=keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x=keras.layers.Conv2D(512,(4,4),padding='same',kernel_initializer=init)(x)\n",
    "    x=InstanceNormalization(axis=-1)(x)\n",
    "    x=keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    #to(16*16*1),N*N為16*16*1\n",
    "    x=keras.layers.Conv2D(1,(4,4),padding='same',kernel_initializer=init)(x) \n",
    "    \n",
    "    model=keras.Model(inputs=x_input,outputs=x)\n",
    "    model.compile(loss='mse',optimizer=keras.optimizers.Adam(lr=0.0002, beta_1=0.5),loss_weights=[0.5])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=discriminator(A[0].shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator NN  \n",
    "![cycleGAN](https://1.bp.blogspot.com/-cj53ochEvkY/XVPRc4F9A0I/AAAAAAAAEbQ/ERiS_nhQpPEDZOOk8SrxmQ6E2CdZFXMTACLcBGAs/s640/%25E6%2593%25B7%25E5%258F%2596.PNG)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(filters,input_layer):\n",
    "    init=keras.initializers.RandomNormal(stddev=0.02) #normal distribution\n",
    "    #1st\n",
    "    x=keras.layers.Conv2D(filters,(3,3),padding='same',kernel_initializer=init)(input_layer)\n",
    "    x=InstanceNormalization(axis=-1)(x)\n",
    "    x=keras.layers.Activation('relu')(x)\n",
    "    #2nd\n",
    "    x=keras.layers.Conv2D(filters,(3,3),padding='same',kernel_initializer=init)(input_layer)\n",
    "    x=InstanceNormalization(axis=-1)(x)\n",
    "    \n",
    "    x=keras.layers.Concatenate()([x,input_layer])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![generator](https://hardikbansal.github.io/CycleGANBlog/images/Generator.jpg)  \n",
    "First downsampling then upsmapling,interpreting the encoding with a number of ResNet layers  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(image_shape,residual_num=1):\n",
    "    init=keras.initializers.RandomNormal(stddev=0.02)\n",
    "    x_input=keras.layers.Input(shape=image_shape)\n",
    "    \n",
    "    #downsampling\n",
    "    #to(256*256*64)\n",
    "    x=keras.layers.Conv2D(64,(7,7),padding='same',kernel_initializer=init)(x_input)\n",
    "    x=InstanceNormalization(axis=-1)(x)\n",
    "    x=keras.layers.Activation('relu')(x)\n",
    "    #to(128*128*128)\n",
    "    x=keras.layers.Conv2D(128,(3,3),strides=(2,2),padding='same',kernel_initializer=init)(x)\n",
    "    x=InstanceNormalization(axis=-1)(x)\n",
    "    x=keras.layers.Activation('relu')(x)\n",
    "    #to(64*64*256)\n",
    "    x=keras.layers.Conv2D(256,(3,3),strides=(2,2),padding='same',kernel_initializer=init)(x)\n",
    "    x=InstanceNormalization(axis=-1)(x)\n",
    "    x=keras.layers.Activation('relu')(x)\n",
    "    \n",
    "    for i in range(residual_num):\n",
    "        x=residual_block(256,x)\n",
    "        \n",
    "    #upsampling\n",
    "    #to(128*128*128)\n",
    "    x=keras.layers.Conv2DTranspose(128,(3,3),strides=(2,2),padding='same',kernel_initializer=init)(x)\n",
    "    x=InstanceNormalization(axis=-1)(x)\n",
    "    x=keras.layers.Activation('relu')(x)\n",
    "    #to(256*256*64)\n",
    "    x=keras.layers.Conv2DTranspose(64,(3,3),strides=(2,2),padding='same',kernel_initializer=init)(x)\n",
    "    x=InstanceNormalization(axis=-1)(x)\n",
    "    x=keras.layers.Activation('relu')(x)\n",
    "    #to(256*256*3)\n",
    "    x=keras.layers.Conv2D(3,(7,7),padding='same',kernel_initializer=init)(x)\n",
    "    x=InstanceNormalization(axis=-1)(x)\n",
    "    x=keras.layers.Activation('tanh')(x)\n",
    "    \n",
    "    model=keras.Model(inputs=x_input,outputs=x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=generator(A[0].shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# composite NN (目的是為了要訓練g_model)\n",
    "假設我們要訓練一個A->B的generator  \n",
    "updated via the weighted sum of four loss function:  \n",
    "* __Adversarial Loss__: Domain-A -> Generator-B -> Domain-B -> Discriminator-B -> \\[real/fake\\]\n",
    "* __Identity Loss__: Domain-B -> Generator-B -> Domain-B\n",
    "* __Forward Cycle Loss__: Domain-A -> Generator-B -> Domain-B -> Generator-A -> Domain-A\n",
    "* __Backward Cycle Loss__: Domain-B -> Generator-A -> Domain-A -> Generator-B -> Domain-B\n",
    "\n",
    "input is Domain-A & Domain-B  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Domain A->Domain B ; g_modelB:image to Domain B\n",
    "def composite(g_modelAtoB,g_modelBtoA,d_model,input_shape): \n",
    "    g_modelAtoB.trainable=True \n",
    "    g_modelBtoA.trainable=False\n",
    "    d_model.trainable=False\n",
    "    \n",
    "    A_input=keras.layers.Input(shape=input_shape)\n",
    "    B_input=keras.layers.Input(shape=input_shape)\n",
    "    #adversarial loss\n",
    "    adversary_output=g_modelAtoB(A_input)\n",
    "    adversary_output=d_model(adversary_output)\n",
    "    #identity loss\n",
    "    identity_output=g_modelAtoB(B_input)\n",
    "    #forward cycle loss\n",
    "    forward_output=g_modelAtoB(A_input)\n",
    "    forward_output=g_modelBtoA(forward_output)\n",
    "    #backward cycle loss\n",
    "    backward_output=g_modelBtoA(B_input)\n",
    "    backward_output=g_modelAtoB(backward_output)\n",
    "    \n",
    "    model=keras.Model(inputs=[A_input,B_input],outputs=[adversary_output,identity_output,forward_output,backward_output])\n",
    "    model.compile(loss=['mse','mae','mae','mae'],optimizer=keras.optimizers.Adam(lr=0.0002, beta_1=0.5),loss_weights=[1,5,10,10])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image_Generator(for Discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_image_generator(dataset,num,patch_shape):\n",
    "    randi=np.random.randint(0,dataset.shape[0],num)\n",
    "    x=dataset[randi]\n",
    "    y=np.ones((num,)+patch_shape)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#圖片生圖片\n",
    "def fake_image_generator(g_model,images,patch_shape): \n",
    "    x=g_model.predict(images)\n",
    "    y=np.zeros((images.shape[0],)+patch_shape)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#會使用舊的image\n",
    "def update_fake_image(pool,images,pool_size=50):\n",
    "    select=[]\n",
    "    for image in images:\n",
    "        if len(pool)<pool_size:\n",
    "            pool.append(image)\n",
    "            select.append(image)\n",
    "        elif np.random.random()<0.5:\n",
    "            select.append(image)\n",
    "        else: #use old\n",
    "            randi=np.random.randint(0,pool_size)\n",
    "            select.append(pool[randi])\n",
    "            pool[randi]=image\n",
    "    return np.asarray(select,'float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要train 4個NN:  \n",
    "![cycleGAN](https://i.imgur.com/OH7Tvpy.png)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(g_modelAtoB,g_modelBtoA,d_modelA,d_modelB,c_modelAtoB,c_modelBtoA,epochs=100,batch_size=1):\n",
    "    batchs=A.shape[0]//batch_size\n",
    "    poolA,poolB=[],[]\n",
    "    patch_shape=d_modelA.output_shape[1:]\n",
    "    for epoch in range(1,epochs+1):\n",
    "        for batch in range(1,batchs+1):\n",
    "            #domain images\n",
    "            realA_x,realA_y=real_image_generator(A,batch_size,patch_shape)\n",
    "            realB_x,realB_y=real_image_generator(B,batch_size,patch_shape)\n",
    "            fakeA_x,fakeA_y=fake_image_generator(g_modelBtoA,realB_x,patch_shape)\n",
    "            fakeB_x,fakeB_y=fake_image_generator(g_modelAtoB,realA_x,patch_shape)\n",
    "            fakeA_x=update_fake_image(poolA,fakeA_x)\n",
    "            fakeB_x=update_fake_image(poolB,fakeB_x)\n",
    "            #update generator_B->A\n",
    "            loss_g_BA,_,_,_,_=c_modelBtoA.train_on_batch(x=[realB_x,realA_x],y=[realA_y,realA_x,realB_x,realA_x])\n",
    "            #update discriminator_A\n",
    "            loss_d1_A=d_modelA.train_on_batch(x=realA_x,y=realA_y)\n",
    "            loss_d2_A=d_modelA.train_on_batch(x=fakeA_x,y=fakeA_y)\n",
    "            #update generator_A->B\n",
    "            loss_g_AB,_,_,_,_=c_modelAtoB.train_on_batch(x=[realA_x,realB_x],y=[realB_y,realB_x,realA_x,realB_x])\n",
    "            #update discriminator_B\n",
    "            loss_d1_B=d_modelB.train_on_batch(x=realB_x,y=realB_y)\n",
    "            loss_d2_B=d_modelB.train_on_batch(x=fakeB_x,y=fakeB_y)\n",
    "            print('epoch{} {}/{} dA[{:.2f} {:.2f}] dB[{:.2f} {:.2f}] gA[{:.2f}] gB[{:.2f}]'.format(epoch,batch,batchs,loss_d1_A,loss_d2_A,loss_d1_B,loss_d2_B,loss_g_BA,loss_g_AB))\n",
    "        if epoch%5==0:\n",
    "            summarize(epoch,g_modelAtoB,g_modelBtoA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save img,save model\n",
    "def summarize(epoch,g_modelAtoB,g_modelBtoA):\n",
    "    in_A,_=real_image_generator(A,5,(0,))\n",
    "    out_B,_=fake_image_generator(g_modelAtoB,in_A,(0,))\n",
    "    in_B,_=real_image_generator(B,5,(0,))\n",
    "    out_A,_=fake_image_generator(g_modelBtoA,in_B,(0,))\n",
    "    in_A=(in_A+1)/2\n",
    "    out_B=(out_B+1)/2\n",
    "    in_B=(in_B+1)/2\n",
    "    out_A=(out_A+1)/2\n",
    "    \n",
    "    plt.figure(figsize=(15,12))\n",
    "    #Domain A-> Domain B\n",
    "    for i in range(5):\n",
    "        plt.subplot(4,5,i+1)\n",
    "        plt.imshow(in_A[i])\n",
    "        plt.axis('off')\n",
    "        plt.title('real A')\n",
    "        plt.subplot(4,5,i+1+5)\n",
    "        plt.imshow(out_B[i])\n",
    "        plt.axis('off')\n",
    "        plt.title('fake B')\n",
    "    #B->A\n",
    "    for i in range(5):\n",
    "        plt.subplot(4,5,10+i+1)\n",
    "        plt.imshow(in_B[i])\n",
    "        plt.axis('off')\n",
    "        plt.title('real B')\n",
    "        plt.subplot(4,5,10+i+1+5)\n",
    "        plt.imshow(out_A[i])\n",
    "        plt.axis('off')\n",
    "        plt.title('fake A')\n",
    "    plt.savefig('cycleGAN horse-zebra\\\\epoch{}.png'.format(epoch))\n",
    "    plt.close()\n",
    "    #save model\n",
    "    g_modelAtoB.save('cycleGAN horse-zebra\\\\generator AtoB epoch{}.h5'.format(epoch))\n",
    "    g_modelBtoA.save('cycleGAN horse-zebra\\\\generator BtoA epoch{}.h5'.format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load A&B from [0,255] to [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=(A-127.5)/127.5\n",
    "B=(B-127.5)/127.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g_modelAtoB=generator(A[0].shape)\n",
    "g_modelBtoA=generator(B[0].shape)\n",
    "d_modelA=discriminator(A[0].shape)\n",
    "d_modelB=discriminator(B[0].shape)\n",
    "c_modelAtoB=composite(g_modelAtoB,g_modelBtoA,d_modelB,A[0].shape)\n",
    "c_modelBtoA=composite(g_modelBtoA,g_modelAtoB,d_modelA,B[0].shape)\n",
    "\n",
    "fit(g_modelAtoB,g_modelBtoA,d_modelA,d_modelB,c_modelAtoB,c_modelBtoA,epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate fake image (style transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(filename):\n",
    "    image=load_img(filename,target_size=(256,256))\n",
    "    image=img_to_array(image) #to ndarray\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
    "custom={'InstanceNormalization':InstanceNormalization}\n",
    "modelAtoB=keras.models.load_model('cycleGAN horse-zebra\\\\generator AtoB epoch1.h5',custom)\n",
    "modelBtoA=keras.models.load_model('cycleGAN horse-zebra\\\\generator BtoA epoch1.h5',custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before=load_image('images.jpg')\n",
    "plt.imshow(before.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before=(before-127.5)/127.5\n",
    "before=np.expand_dims(before,axis=0)\n",
    "after=modelAtoB.predict(before)\n",
    "plt.imshow((after[0]+1)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruct=modelBtoA.predict(after)\n",
    "plt.imshow((reconstruct[0]+1)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (tensorflow-gpu 2.0)",
   "language": "python",
   "name": "other-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
